Links to things I have mentioned in the talk and others:

Attention Is All You Need: https://arxiv.org/abs/1706.03762

Improving Language Understanding: https://s3-us-west-2.amazonaws.com/openai-assets/research-covers/language-unsupervised/language_understanding_paper.pdf

Language Models are Few-Shot Learners: https://proceedings.neurips.cc/paper/2020/file/1457c0d6bfcb4967418bfb8ac142f64a-Paper.pdf

New Yorker article on Danny Dunn and the Homework Machine: https://www.newyorker.com/books/second-read/what-a-sixty-five-year-old-book-teaches-us-about-ai

ELIZA paper: http://www.universelle-automation.de/1966_Boston.pdf

Large Language Models and the Reverse Turing Test: https://arxiv.org/abs/2207.14382

Racter on archive.org: https://archive.org/details/msdos_Racter_1984

Policeman's Beard book: https://archive.org/details/policemansbeardi0000unse

BrexHQ prompt engineering doc: https://github.com/brexhq/prompt-engineering

Hugging Face chat: https://huggingface.co/chat/

llama.cpp: https://github.com/ggerganov/llama.cppx
